# LinkedIn Post Analyzer

KI-gestÃ¼tzte LinkedIn-Post-Analyse mit Sentiment-Bewertung und Executive Summary.  
Gebaut mit **FastAPI + Apify + Claude** â€“ ohne Fixkosten, pay-per-use.

## Features

- ğŸ” **Keyword-Scraping** via Apify (LinkedIn Posts der letzten 7 / 14 / 30 Tage)
- ğŸ§¹ **Automatische Deduplizierung** Ã¼ber mehrere Suchbegriffe
- ğŸ¤– **Sentiment-Analyse** je Post + Kommentare (Claude Haiku)
- ğŸ“Š **Executive Summary** mit Stimmungsbild & MeinungsfÃ¼hrern (Claude Sonnet)
- ğŸ“„ **HTML-Report** â€“ direkt im Browser, zum Download oder lokal gespeichert
- ğŸ–¥ï¸ **Web-UI** â€“ Formular im Browser, Echtzeit-Fortschritt via SSE
- âŒ¨ï¸ **CLI-Modus** â€“ vollstÃ¤ndig skriptbar fÃ¼r Automatisierung

## Kosten (pay-per-use, keine Fixkosten)

| Komponente | Kosten / Auswertung |
|---|---|
| Apify LinkedIn Scraping (100 Posts) | ~0,50â€“1,50 â‚¬ |
| Claude Haiku â€“ Sentiment je Post | ~0,10â€“0,30 â‚¬ |
| Claude Sonnet â€“ Executive Summary | ~0,05â€“0,15 â‚¬ |
| **Gesamt** | **~0,65â€“2,00 â‚¬** |

## Schnellstart

### 1. Repository klonen & Umgebungsvariablen setzen

```bash
git clone https://github.com/DEIN-USERNAME/linkedin-analyzer.git
cd linkedin-analyzer
cp .env.example .env
# .env Ã¶ffnen und API-Keys eintragen
```

### 2. Web-Server starten

```bash
docker compose up -d
```

Ã–ffne `http://localhost:8080` im Browser.

### 3. CLI-Modus (einmalige Auswertung â†’ HTML-Datei)

```bash
docker run --rm \
  --env-file .env \
  -v $(pwd)/output:/output \
  linkedin-analyzer:latest \
  python -m app.main analyze \
  --keywords "Agentic AI, KI Plattform" \
  --days 7 \
  --max-posts 30 \
  --output /output/analyse_2025-05.html
```

Der Report liegt danach in `./output/analyse_2025-05.html`.

## Web-UI Zugriffsschutz (HTTP Basic Auth)

Das Web-Frontend lÃ¤sst sich per **HTTP Basic Auth** absichern, damit keine unbefugten Dritten auf Kosten des Betreibers Analysen auslÃ¶sen kÃ¶nnen.

### Umgebungsvariablen

| Variable | Pflicht | Beschreibung |
|---|---|---|
| `WEB_USER` | Nein | Benutzername fÃ¼r den Web-Login |
| `WEB_PASSWORD` | Nein | Passwort fÃ¼r den Web-Login |

**Verhalten:**
- Sind **beide** Variablen gesetzt â†’ Browser zeigt Login-Dialog, alle Analyse-Endpunkte sind geschÃ¼tzt.
- Ist eine der Variablen leer oder nicht gesetzt â†’ Auth ist **deaktiviert** (z. B. fÃ¼r lokale Entwicklung).
- Der `/health`-Endpunkt ist immer ohne Auth erreichbar (wird vom Docker-Healthcheck benÃ¶tigt).

### Konfiguration in `.env`

```env
WEB_USER=meinuser
WEB_PASSWORD=sicheres-passwort
```

### Konfiguration per `docker run`

```bash
docker run --rm \
  -e WEB_USER=meinuser \
  -e WEB_PASSWORD=sicheres-passwort \
  -e APIFY_TOKEN=... \
  -e ANTHROPIC_API_KEY=... \
  -p 8080:8080 \
  linkedin-analyzer:latest
```

---

## Einrichtung

### API-Keys

**Apify**
1. Account anlegen: https://apify.com (kostenloser Einstieg mit Credits)
2. `Settings â†’ Integrations â†’ API Tokens` â†’ Token kopieren
3. In `.env`: `APIFY_TOKEN=apify_api_...`

**Anthropic**
1. Console: https://console.anthropic.com â†’ API Keys â†’ Neuen Key erstellen
2. In `.env`: `ANTHROPIC_API_KEY=sk-ant-...`

### LinkedIn Session-Cookie (Apify)

Apify benÃ¶tigt einen LinkedIn-Session-Cookie fÃ¼r das Scraping:
1. Im Browser auf linkedin.com einloggen
2. DevTools Ã¶ffnen (`F12`) â†’ Application â†’ Cookies â†’ `li_at` kopieren
3. Im Apify-Dashboard beim Actor als Input-Cookie hinterlegen

Der Cookie hÃ¤lt mehrere Wochen. Apify kÃ¼mmert sich um die Rotation.

### Apify Actor

Der Standard-Actor ist `harvestapi~linkedin-post-search` (kein LinkedIn-Cookie erforderlich).

| Variable | Standardwert | Beschreibung |
|---|---|---|
| `APIFY_ACTOR` | `harvestapi~linkedin-post-search` | Verwendeter Apify Actor |

**Input-Schema** (`harvestapi~linkedin-post-search`):

```json
{
  "searchQueries": ["b2b sales"],
  "maxPosts": 20,
  "scrapeComments": false,
  "scrapeReactions": false,
  "maxReactions": 5
}
```

Die App setzt `scrapeComments` automatisch anhand der Web-UI-Option â€Kommentare einbeziehen". `scrapeReactions` und `maxReactions` sind fest auf `false` / `5` gesetzt, um Kosten zu minimieren.

Falls du einen anderen Actor nutzen mÃ¶chtest:

```env
APIFY_ACTOR=jiri.spilka~linkedin-post-scraper
```

VerfÃ¼gbare Actors im [Apify Marketplace](https://apify.com/store).

## GitHub Actions / CI-CD

Der Workflow in `.github/workflows/deploy.yml` baut das Docker-Image bei jedem Push auf `main` und deployed automatisch auf deinen Server.

### Secrets in GitHub eintragen

| Secret | Wert |
|---|---|
| `DEPLOY_HOST` | IP oder Hostname deines Servers |
| `DEPLOY_USER` | SSH-Benutzername |
| `DEPLOY_SSH_KEY` | Privater SSH-Key (ohne Passphrase) |

Die API-Keys (`APIFY_TOKEN`, `ANTHROPIC_API_KEY`) werden **nicht** als GitHub Secrets gesetzt â€“ sie liegen als `.env`-Datei direkt auf dem Server und werden nie committet.

### Server vorbereiten

```bash
# Auf dem Server (einmalig)
git clone https://github.com/DEIN-USERNAME/linkedin-analyzer.git ~/linkedin-analyzer
cd ~/linkedin-analyzer
cp .env.example .env && nano .env   # API-Keys eintragen
```

Danach Ã¼bernimmt der GitHub Actions Workflow das Deploy automatisch.

## Projektstruktur

```
linkedin-analyzer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI App + CLI-Entrypoint
â”‚   â”œâ”€â”€ analyzer.py      # Apify + Claude Logik, Datenmodelle
â”‚   â”œâ”€â”€ report.py        # HTML-Report-Generator
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html   # Web-UI
â”œâ”€â”€ output/              # CLI-Reports (gitignored)
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml   # CI/CD Pipeline
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Lokale Entwicklung (ohne Docker)

```bash
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env && nano .env  # Keys eintragen

# Server mit Auto-Reload
uvicorn app.main:app --reload --port 8080
```

## Lizenz

MIT
